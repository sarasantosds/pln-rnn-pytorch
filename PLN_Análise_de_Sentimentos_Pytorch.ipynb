{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOW1/5+25k01P2M7/2glGY1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processamento de linguagem natural com RNN usando Pytorch"
      ],
      "metadata": {
        "id": "TekA-AkUnd2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "OsJcUaWcTyfz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_XF0wMTqJDA",
        "outputId": "c3b904df-b965-4902-9bc5-fd877e228ca4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-30 01:07:48--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  29.2MB/s    in 2.7s    \n",
            "\n",
            "2025-01-30 01:07:51 (29.2 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_EPOCHS = 5"
      ],
      "metadata": {
        "id": "ttOFRDq1dmxo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "3uLxbGF-dxm3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "O91bmEfMd_ka"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = re.sub(r'[^\\w\\s]', '',text.lower().strip())\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "DCzpQxjbgMYK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.samples = []\n",
        "        for label in ['pos', 'neg']:\n",
        "            dir_path = os.path.join(data_dir, label)\n",
        "            if not os.path.exists(dir_path):\n",
        "                raise FileNotFoundError(f\"Diretório {dir_path} não encontrado.\")\n",
        "            for filename in os.listdir(dir_path):\n",
        "                file_path = os.path.join(dir_path, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        text = file.read()\n",
        "                        self.samples.append((text, 1.0 if label == 'pos' else 0.0))\n",
        "        print(f\"{len(self.samples)} amostras carregadas para {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.samples[idx]\n",
        "        tokens = preprocess_text(text)\n",
        "        return tokens, torch.tensor(label, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "PaUKh3z7gQuS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(dataset):\n",
        "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    idx = 2\n",
        "    for i in range(len(dataset)):\n",
        "        tokens, _ = dataset[i]\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = idx\n",
        "                idx += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "FqXsrIjMldmG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(tokens, vocab):\n",
        "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]"
      ],
      "metadata": {
        "id": "8CJXWjAPoi5c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    indices_list = [torch.tensor(text_to_indices(tokens, vocab), dtype=torch.long) for tokens in texts]\n",
        "    padded_texts = nn.utils.rnn.pad_sequence(indices_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return padded_texts.to(device), torch.tensor(labels, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "yF2rmufto3rG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = \"aclImdb/train\"\n",
        "test_data_dir =  \"aclImdb/test\"\n",
        "\n",
        "train_dataset = IMDBDataset(train_data_dir)\n",
        "valid_dataset = IMDBDataset(test_data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqih67WepNHv",
        "outputId": "b255c7cb-f727-49b2-fc26-9e5815d0896e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 amostras carregadas para aclImdb/train\n",
            "25000 amostras carregadas para aclImdb/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(train_dataset)"
      ],
      "metadata": {
        "id": "XpopI3uSpZT1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "HDbL5yfbpvEe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = hidden[-1]\n",
        "        output = self.fc(hidden)\n",
        "        return self.sigmoid(output)\n"
      ],
      "metadata": {
        "id": "Rfwn1PTyp26y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab)\n",
        "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)"
      ],
      "metadata": {
        "id": "Ixk-C4rCtxPJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss().to(device)"
      ],
      "metadata": {
        "id": "Lz5vzd6Ytz7T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for text, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "gd8vrphet2kl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text, labels in dataloader:\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "HPfxVOd7t730"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "    print(f'Epoch {epoch+1}/{N_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqysu8qut-NN",
        "outputId": "c1d9b33e-4592-4771-945a-0741ae9e3745"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 0.694 | Val Loss: 0.693\n",
            "Epoch 2/5 | Train Loss: 0.693 | Val Loss: 0.693\n",
            "Epoch 3/5 | Train Loss: 0.693 | Val Loss: 0.693\n",
            "Epoch 4/5 | Train Loss: 0.692 | Val Loss: 0.693\n",
            "Epoch 5/5 | Train Loss: 0.691 | Val Loss: 0.693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AyGxVVQVuJ9j"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}